# ğŸ¤– Agent 8 - RL Trading Gold (XAUUSD)

> **Reinforcement Learning Agent for Mean Reversion Trading on Gold (XAUUSD) - M15 Timeframe**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Stable-Baselines3](https://img.shields.io/badge/SB3-2.0%2B-green.svg)](https://stable-baselines3.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-In%20Development-orange.svg)]()

---

## ğŸ“‹ Overview

**Agent 8** is a sophisticated Reinforcement Learning trading agent specializing in **mean reversion strategies** on the Gold (XAUUSD) market using **M15 (15-minute) timeframe**. Built with institutional-grade standards, it leverages **PPO (Proximal Policy Optimization)** and incorporates hedge fund-level techniques including:

- âœ… **Demonstration Learning** (curriculum-based training)
- âœ… **Adaptive Entropy Scheduling** (0.40 â†’ 0.20)
- âœ… **Protected Reward Shaping** (+5.0 trading action rewards)
- âœ… **Risk Management** (FTMO-compliant)
- âœ… **100+ Technical Features** (RSI, MACD, ADX, COT data, macro events)

---

## ğŸ¯ Strategy

**Type**: Mean Reversion
**Timeframe**: M15 (15 minutes)
**Market**: Gold (XAUUSD)

**Core Logic**:
- **Entry**: Oversold (RSI < 40) â†’ BUY, Overbought (RSI > 60) â†’ SELL
- **Exit**: Fixed TP (4R ~1%) or Fixed SL (ATR-based)
- **Hold Time**: 15 minutes - 4 hours (quick reversions)

**Risk Management**:
- Max Drawdown: < 10% (FTMO compliant)
- Daily Loss Limit: < 5%
- Position Sizing: Kelly Criterion
- Over-Trading Protection: Max 1 trade per 2.5 hours

---

## ğŸš€ Features

### 1. Advanced RL Architecture
- **Algorithm**: PPO (Proximal Policy Optimization)
- **Action Space**: Discrete(3) - [0=SELL, 1=HOLD, 2=BUY]
- **Network**: [512, 512] neurons
- **Training**: 500K steps validation, 1M+ production

### 2. Institutional-Grade Reward System
- **Trading Action Rewards**: +5.0 (open), +5.0 (profitable close), -1.0 (loss)
- **Bonuses**: Ã—20 amplified (direction prediction, profit taking, loss cutting)
- **HOLD Penalty**: Exponential (-18.0 max after 20 consecutive holds)
- **Protected Rewards**: Added AFTER scaling (not diluted)

### 3. Demonstration Learning (Revolutionary!)
- **Phase 1** (0-100K): Force 100% of smart trades (RSI signals) + MEGA rewards (+10.0)
- **Phase 2** (100K-300K): Reduce forcing (50% â†’ 0%), rewards (+5.0)
- **Phase 3** (300K-500K): Full autonomy, amplified rewards (+2.0)

### 4. Anti-Mode Collapse Mechanisms
- **Adaptive Entropy**: 0.40 (high exploration) â†’ 0.20 (moderate)
- **Action Masking**: Block if â‰¥5 times in last 10 actions
- **Forced Trading**: Safety net if 0 trades after 1000 steps
- **Diversity Rewards**: Shannon entropy-based

### 5. ALL Features (No Selection - Full Dataset)
**Technical Indicators**:
- Price Action: SMA, EMA, Bollinger Bands
- Momentum: RSI, MACD, ADX, Stochastic
- Volatility: ATR, Historical Volatility

**Correlations**:
- Gold vs EURUSD, USDJPY, DXY, USDCHF, AUDJPY, Silver

**Macro/Fundamental**:
- COT Data (CFTC): Noncomm/Comm positions, divergence
- US Macro Events: FOMC, NFP, CPI, PPI
- Seasonality: Strong months, best months (Seasonax)

---

## ğŸ“Š Performance Targets

**Validation (500K steps)**:
- Total Trades: > 100
- Action Diversity: ~30% SELL, ~30% HOLD, ~30% BUY
- Entropy: > 0.20

**Production (1M+ steps)**:
- ROI: 10-15% (test period 2022-2024)
- Sharpe Ratio: > 1.2
- Max Drawdown: < 8%
- Win Rate: > 50%
- Profit Factor: > 1.5

**FTMO Compliance**:
- Max Drawdown: < 10% âœ…
- Daily Loss: < 5% âœ…
- Minimum Trading Days: 4+ âœ…

---

## ğŸ› ï¸ Installation

### Requirements
```bash
Python 3.8+
NumPy >= 1.20
Pandas >= 1.3
Stable-Baselines3 >= 2.0
Gymnasium >= 0.28
TensorBoard (optional, for monitoring)
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Data Requirements
- **XAUUSD M15**: 2008-2025 (~300,000 bars)
- **XAUUSD H1**: 2008-2025 (correlations)
- **XAUUSD D1**: 2008-2025 (trends)
- **Correlations**: EURUSD, USDJPY, DXY, etc. (same period)
- **COT Data**: Gold Futures weekly positions (CFTC)
- **US Macro Events**: FOMC, NFP, CPI (2008-2025)

---

## ğŸš€ Quick Start

### 1. Training (500K Validation - 40 min)
```bash
cd "AGENT 8 UNIQUEMENT"
python train.py
```

**Outputs**:
- `checkpoints/agent8_checkpoint_50000_steps.zip` (every 50K)
- `checkpoints/agent8_500k_final.zip` (final model)
- `checkpoints_analysis/checkpoint_*.csv` (stats)
- `training_summary_500k.json`

### 2. Interview Diagnostic (3 min)
```bash
python interview.py
```

Asks 8 critical questions to diagnose agent behavior:
1. Are fixes activated?
2. Is Demonstration Learning forcing trades?
3. Are Trading Action Rewards applied?
4. Why does agent choose HOLD?
5. Are logits balanced?
6. Is entropy correct?
7. What does agent see in observations?
8. What's the ROOT CAUSE?

**Output**: `DIAGNOSTIC_REPORT_V27_YYYYMMDD_HHMMSS.txt`

### 3. Backtest (Production Model)
```bash
python backtest.py --model checkpoints/agent8_500k_final.zip --start 2022-01-01 --end 2024-12-31
```

---

## ğŸ“‚ Project Structure

```
AGENT 8 UNIQUEMENT/
â”œâ”€â”€ trading_env.py              # Main environment (7 NUCLEAR fixes)
â”œâ”€â”€ train.py                    # Training script (PPO 500K)
â”œâ”€â”€ interview.py                # Diagnostic tool (8 questions)
â”œâ”€â”€ backtest.py                 # Backtesting script
â”œâ”€â”€ RUN_TRAINING.bat            # Training launcher (Windows)
â”œâ”€â”€ RUN_INTERVIEW.bat           # Interview launcher (Windows)
â”œâ”€â”€ checkpoints/                # Saved models
â”œâ”€â”€ checkpoints_analysis/       # Training stats (CSV)
â”œâ”€â”€ V2.7_CHANGES.md             # 7 NUCLEAR fixes documentation
â”œâ”€â”€ V2.7_CRITICAL_FIXES_APPLIED.md  # Latest fixes (3 critical)
â”œâ”€â”€ DIAGNOSTIC_URGENT.md        # Current issues & hypotheses
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”¬ Methodology

### Inspired by Hedge Funds Standards

**Renaissance Technologies**:
- Adaptive entropy scheduling
- Curriculum learning (easy â†’ hard)
- Feature selection (SHAP analysis)

**Two Sigma**:
- Real-time monitoring (TensorBoard)
- Behavioral diversity enforcement (Shannon entropy)
- Anti-mode collapse mechanisms

**Citadel**:
- Constrained exploration (entropy limits)
- Risk management (Kelly Criterion, VaR)
- Demonstration-based learning

### Academic References

1. **Haarnoja et al. (2018)** - "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL"
2. **Hester et al. (2018)** - "Deep Q-learning from Demonstrations"
3. **Narvekar et al. (2020)** - "Curriculum Learning for Reinforcement Learning Domains"
4. **Ng et al. (1999)** - "Policy Invariance Under Reward Transformations"

---

## ğŸ› Bugs RÃ©solus (2025-12-02)

### Bug 1 : Early Returns dans `_calculate_reward()` âœ… RÃ‰SOLU

**ProblÃ¨me** : Les `return` statements bypassaient le reward +5.0 pour les trades.

**Solution** : ChangÃ© tous les `return X` en `reward += X` pour accumuler.

**Fichier** : `environment/trading_env.py`

---

### Bug 2 : prices_df utilisant mauvaises donnÃ©es âœ… RÃ‰SOLU

**ProblÃ¨me** : `df_full` ne contient que des colonnes `_close` (EURUSD ~$1.5), pas de vraies donnÃ©es OHLCV Gold.

**Solution** : Utiliser `auxiliary_data['xauusd_raw']['H1']` pour les prix rÃ©els Gold (~$1000).

**Fichiers** :
- `analysis/interview_env_only.py`
- `analysis/quick_diagnostic.py`
- `training/train_smoke_test.py`

---

### Bug 3 : FIX 8 bloquant TOUS les trades âœ… RÃ‰SOLU

**ProblÃ¨me** : `last_trade_open_step` n'Ã©tait pas reset dans `reset()`, causant des diffÃ©rences nÃ©gatives :
```
current_step: 44290
last_trade_open_step: 61667  (de l'Ã©pisode prÃ©cÃ©dent!)
DiffÃ©rence: -17377 bars
-17377 < 10 = TRUE â†’ TOUS LES TRADES BLOQUÃ‰S
```

**Solution** : AjoutÃ© dans `reset()` :
```python
self.last_trade_open_step = -10  # Allow first trade immediately
self.position_opened_this_step = False
self.position_closed_this_step = False
self.last_closed_pnl = 0.0
self.demonstration_trade_this_step = False
```

**Fichier** : `environment/trading_env.py` (lignes 398-407)

---

## ğŸ”¬ Scripts de Diagnostic

### 1. Interview Trades (`analysis/interview_trades.py`)

**Le plus complet** - Pose 5 questions critiques :

| Question | But |
|----------|-----|
| Q1 | Les positions S'OUVRENT-elles ? |
| Q2 | Le TP/SL est-il ATTEIGNABLE ? |
| Q3 | Que se passe-t-il sur 100 steps ? |
| Q4 | FIX 8 (Over-Trading) bloque-t-il ? |
| Q5 | Simulation longue (500 steps) |

```bash
python analysis/interview_trades.py
```
**DurÃ©e** : ~3 minutes

### 2. Interview Env Only (`analysis/interview_env_only.py`)

Test l'environnement **sans modÃ¨le** - VÃ©rifie fixes V2.7, rewards, `_open_position()`.

### 3. Quick Diagnostic (`analysis/quick_diagnostic.py`)

10 actions BUY manuelles + test direct `_open_position()`.

### 4. Check Prices (`analysis/check_prices.py`)

VÃ©rifie que les prix sont bien Gold (~$800-2000) et pas EURUSD (~$1.5).

### 5. Behavioral Analysis (`analysis/behavioral_analysis.py`) ğŸ†•

**Psychanalyse de l'agent** - 8 questions comportementales :

| Question | Analyse |
|----------|---------|
| Q1 | Distribution des actions (BUY/SELL/HOLD %) |
| Q2 | Peur du risque? (prÃ©fÃ¨re HOLD vs trade) |
| Q3 | Ferme-t-il ses positions? |
| Q4 | Rewards reÃ§us (positifs/nÃ©gatifs) |
| Q5 | Contexte marchÃ© quand il trade |
| Q6 | Ce qu'il "voit" (observations) |
| Q7 | Ã‰volution sur 1000 steps |
| Q8 | SynthÃ¨se + hypothÃ¨ses |

```bash
python analysis/behavioral_analysis.py
```
**DurÃ©e** : ~5 minutes
**Output** : Diagnostic comportemental avec profil (PASSIF/ACTIF/BLOQUÃ‰) et recommandations

### 6. Check Q-Values / Policy (`analysis/check_qvalues.py`) ğŸ†•

**Regarde ce que l'agent "pense"** - ProbabilitÃ©s d'action du policy network :

```bash
python analysis/check_qvalues.py
python analysis/check_qvalues.py --model models/best_model.zip
python analysis/check_qvalues.py --n_samples 100
```

**Diagnostics**:
- Si P(HOLD) >> P(BUY) et P(SELL) â†’ L'agent a appris que ne rien faire est "safe"
- Calcul de l'entropy pour vÃ©rifier exploration
- Mode collapse detection

### 7. Check Reward Function (`analysis/check_reward_function.py`) ğŸ†•

**Checklist reward function** - 4 questions critiques :

| Question | Check |
|----------|-------|
| A | HOLD donne-t-il un reward (mÃªme petit)? |
| B | Trades perdants TROP pÃ©nalisÃ©s? |
| C | Reward UNIQUEMENT Ã  la clÃ´ture? |
| D | Risk/Reward encouragÃ©? |

```bash
python analysis/check_reward_function.py
```

**PIÃˆGE CLASSIQUE dÃ©tectÃ©** : Si reward qu'Ã  clÃ´ture + pertes pÃ©nalisÃ©es â†’ Agent apprend "ne jamais ouvrir = jamais de perte = SAFE"

### 8. Full Diagnostic (`analysis/full_diagnostic.py`) ğŸ†•

**Diagnostic COMPLET** - 6 tests en un script :

| Test | Description |
|------|-------------|
| 1 | Distribution des actions (HOLD/BUY/SELL %) |
| 2 | Random vs Trained comparison |
| 3 | Observations normalization check |
| 4 | Reward per action type |
| 5 | Exploration (entropy) analysis |
| 6 | Positions analysis |

```bash
python analysis/full_diagnostic.py
python analysis/full_diagnostic.py --model models/best_model.zip --episodes 20
```

**Output** : SynthÃ¨se avec problÃ¨mes dÃ©tectÃ©s + solutions recommandÃ©es

---

## ğŸ”„ Workflow de Diagnostic

```
1. ProblÃ¨me dÃ©tectÃ© (0 trades)
         â†“
2. Lancer interview_trades.py
         â†“
3. Identifier la question qui Ã©choue (Q1-Q5)
         â†“
4. Analyser le code source correspondant
         â†“
5. Appliquer le fix
         â†“
6. Relancer interview pour vÃ©rifier
         â†“
7. Lancer smoke test (10K steps)
         â†“
8. Si SUCCESS â†’ Training complet (500K+ steps)
```

---

## ğŸ“¦ Repositories

| Repo | Description |
|------|-------------|
| [AGENT-8-UNIQUEMENT-](https://github.com/tradingluca31-boop/AGENT-8-UNIQUEMENT-) | Code Agent 8 centralisÃ© |
| [AMELIORATION-AGENT-SCRIPT](https://github.com/tradingluca31-boop/AMELIORATION-AGENT-SCRIPT) | Scripts d'amÃ©lioration & diagnostic |

---

## ğŸ¤ Contributing

This is a personal research project. Contributions, suggestions, and discussions are welcome!

**Areas for Improvement**:
1. Resolve 0 trades issue (reward scale, demonstration learning)
2. Hyperparameter tuning (Optuna, Ray Tune)
3. Multi-timeframe fusion (M15 + H1 + D1)
4. Ensemble learning (combine with other agents)
5. Walk-forward optimization
6. Live trading integration (MetaTrader 5 API)

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ“§ Contact

For questions, feedback, or collaboration:
- **GitHub Issues**: [Report a bug or request a feature]
- **Email**: [Your email if you want]

---

## ğŸ™ Acknowledgments

- **Stable-Baselines3** team for excellent RL library
- **OpenAI Gym/Gymnasium** for standardized environments
- **CFTC** for COT data
- **Seasonax** for seasonality data
- **MetaQuotes** for MetaTrader 5 platform

---

## âš ï¸ Disclaimer

**THIS IS RESEARCH SOFTWARE - NOT FINANCIAL ADVICE**

Trading financial instruments involves substantial risk of loss. Past performance is not indicative of future results. This agent is **experimental** and should **NOT** be used with real money without extensive testing, validation, and understanding of the risks involved.

**Use at your own risk. The authors assume no responsibility for financial losses.**

---

---

## ğŸ“œ Changelog

### [2025-12-02] - Session 3
- âœ… **ADD** `analysis/normalization_audit.py` - Audit Wall Street grade (47 features Ã  normaliser)
- âœ… **ADD** 7 MEMORY features dans `trading_env.py` (like Agent 7)
  - win_rate, streak, avg_pnl, best, worst, win_count, loss_count
- âœ… **ADD** `analysis/normalization_audit_summary.json` - RÃ©sultats audit
- âœ… **FIX** `normalize_features_wallstreet()` - Normalisation automatique dans trading_env.py
  - PRICE RAW: Min-Max [0, 1]
  - RSI/Stoch: Divide by 100
  - VOLUME: Log + Z-score [-3, 3]
  - ATR: Percentile rank [0, 1]
  - MACD/ADX: Z-score [-3, 3]
- ğŸ” **DIAGNOSTIC** 31 features CRITIQUES identifiÃ©es (prix raw 0-2000)
- ğŸ“Š **Observation space**: 209 base + 20 RL = **229 features** total

### [2025-12-02] - Session 2
- âœ… **ADD** `analysis/behavioral_analysis.py` - Psychanalyse de l'agent (8 questions)
- âœ… **ADD** `analysis/check_qvalues.py` - VÃ©rification Q-values/Policy output
- âœ… **ADD** `analysis/check_reward_function.py` - Checklist reward function (4 questions)
- âœ… **ADD** `analysis/full_diagnostic.py` - Diagnostic COMPLET (6 tests)
- âœ… **FIX** Bug 3 - FIX 8 reset (`last_trade_open_step` non reset dans `reset()`)
- âœ… **VERIFIED** Interview trades: 50 positions ouvertes, 48 fermÃ©es

### [2025-12-02] - Session 1
- âœ… **FIX** Bug 1 - Early returns dans `_calculate_reward()` bypassaient rewards +5.0
- âœ… **FIX** Bug 2 - `prices_df` utilisait EURUSD (~$1.5) au lieu de Gold (~$2000)
- âœ… **ADD** `analysis/interview_trades.py` - Diagnostic 5 questions
- âœ… **ADD** `analysis/interview_env_only.py` - Test environnement sans modÃ¨le
- âœ… **ADD** `analysis/quick_diagnostic.py` - Test rapide 10 actions
- âœ… **ADD** `analysis/check_prices.py` - VÃ©rification prix Gold vs EURUSD

### [2025-12-01] - Initial Setup
- ğŸš€ **INIT** Structure projet Agent 8
- ğŸ“¦ **ADD** `environment/trading_env.py` - GoldTradingEnvAgent8 V2.7
- ğŸ“¦ **ADD** `training/train_smoke_test.py` - Smoke test 10K steps

---

**Last Updated**: 2025-12-02
**Version**: V2.9 NORMALIZED (229 features - Wall Street grade normalization applied)
**Status**: âœ… Ready for training - All features normalized automatically
